{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An RNN model to generate sequences\n",
    "RNN models can generate long sequences based on past data. This can be used to predict stock markets, temperatures, traffic or sales data based on past patterns. They can also be adapted to [generate text](https://docs.google.com/presentation/d/18MiZndRCOxB7g-TcCl2EZOElS5udVaCuxnGznLnmOlE/pub?slide=id.g139650d17f_0_1185). The quality of the prediction will depend on training data, network architecture, hyperparameters, the distance in time at which you are predicting and so on. But most importantly, it will depend on wether your training data contains examples of the behaviour patterns you are trying to predict.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "This is the solution file. The corresponding tutorial file is [01_RNN_generator_playground.ipynb](01_RNN_generator_playground.ipynb)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import utils_prettystyle\n",
    "import utils_batching\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load temperatures (Seattle, university of WA campus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = np.genfromtxt(open(\"temperatures/univ_of_wa_temperatures.csv\", \"rb\"), delimiter=\",\", skip_header=True,\n",
    "                      usecols=[1,2,3], converters = {1: lambda s: np.datetime64(s)})\n",
    "dates = temperatures[:]['f0']\n",
    "min_temps = temperatures[:]['f1']\n",
    "max_temps = temperatures[:]['f2']\n",
    "\n",
    "DATA_SEQ_LEN = max_temps.shape[0]\n",
    "data = np.nan_to_num(max_temps[0:17010]) ## Hack, must clean the data properly. Also, skipping dates !!\n",
    "data = np.reshape(data, [-1, 3])\n",
    "data = np.mean(data, axis=1)\n",
    "#plt.plot(dates[0:700], data[0:700])\n",
    "plt.plot(data)\n",
    "plt.show()\n",
    "print(data[0:2000])\n",
    "print(dates[0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_CELLSIZE = 100  # size of the RNN cells\n",
    "NLAYERS = 3         # number of stacked RNN cells (needed for tensor shapes but code must be changed manually)\n",
    "SEQLEN = 128         # unrolled sequence length\n",
    "BATCHSIZE = 32      # mini-batch size\n",
    "DROPOUT_PKEEP = 0.7 # probability of neurons not being dropped (should be between 0.5 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training sequences\n",
    "This is what the neural network will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function dumb_minibatch_sequencer splits the data into batches of sequences sequentially.\n",
    "for samples, labels, epoch in utils_batching.rnn_minibatch_sequencer(data, BATCHSIZE, SEQLEN, nb_epochs=1):\n",
    "    break\n",
    "print(\"Sample shape: \" + str(samples.shape))\n",
    "print(\"Label shape: \" + str(labels.shape))\n",
    "print(\"Excerpt from first batch:\")\n",
    "subplot = 231\n",
    "for i in range(6):\n",
    "    plt.subplot(subplot)\n",
    "    plt.plot(samples[i])\n",
    "    subplot += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model definition\n",
    "When executed, this function instantiates the Tensorflow graph for our model.\n",
    "![deep RNN schematic](images/deep_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rnn_fn(features, Hin, labels, dropout_pkeep):\n",
    "    X = features\n",
    "    batchsize = tf.shape(X)[0]\n",
    "    seqlen = tf.shape(X)[1]\n",
    "    \n",
    "    cells = [tf.nn.rnn_cell.GRUCell(RNN_CELLSIZE, activation=tf.nn.relu) for _ in range(NLAYERS)]\n",
    "    # dropout useful between cell layers only: no output dropout on last cell\n",
    "    dcells = [tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = dropout_pkeep) for cell in cells[:-1]]\n",
    "    dcells.append(cells[-1])\n",
    "    # a stacked RNN cell still works like an RNN cell\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(dcells, state_is_tuple=False)\n",
    "    # X[BATCHSIZE, SEQLEN, 1], Hin[BATCHSIZE, RNN_CELLSIZE*NLAYERS]\n",
    "    # the sequence unrolling happens here\n",
    "    Yn, H = tf.nn.dynamic_rnn(cell, X, initial_state=Hin, dtype=tf.float32)\n",
    "    # Yn[BATCHSIZE, SEQLEN, RNN_CELLSIZE]\n",
    "    Yn = tf.reshape(Yn, [batchsize*seqlen, RNN_CELLSIZE])\n",
    "    Yr = tf.layers.dense(Yn, 1) # Yr [BATCHSIZE*SEQLEN, 1]\n",
    "    Yr = tf.reshape(Yr, [batchsize, seqlen, 1]) # Yr [BATCHSIZE, SEQLEN, 1]\n",
    "    Yout = Yr[:,-1,:] # Last output Yout [BATCHSIZE, 1]\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(Yr, labels) # labels[BATCHSIZE, SEQLEN, 1]\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    return Yout, H, loss, train_op, Yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for inputs\n",
    "Hin = tf.placeholder(tf.float32, [None, RNN_CELLSIZE * NLAYERS])\n",
    "samples = tf.placeholder(tf.float32, [None, None, 1]) # [BATCHSIZE, SEQLEN, 1]\n",
    "labels = tf.placeholder(tf.float32, [None, None, 1]) # [BATCHSIZE, SEQLEN, 1]\n",
    "dropout_pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# instantiate the model\n",
    "Yout, H, loss, train_op, Yr = model_rnn_fn(samples, Hin, labels, dropout_pkeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "This is a generative model: run one trained RNN cell in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_run(prime_data, run_length):\n",
    "    H_ = np.zeros([1, RNN_CELLSIZE * NLAYERS]) # zero state initially\n",
    "    Yout_ = np.zeros([1, 1])\n",
    "    data_len = prime_data.shape[0]\n",
    "\n",
    "    # prime the state from data\n",
    "    if data_len > 0:\n",
    "        Yin = np.array(prime_data)\n",
    "        Yin = np.reshape(Yin, [1, data_len, 1]) # reshape as one sequence\n",
    "        feed = {Hin: H_, samples: Yin, dropout_pkeep: 1.0} # no dropout during inference\n",
    "        Yout_, H_ = sess.run([Yout, H], feed_dict=feed)\n",
    "    \n",
    "    # run prediction\n",
    "    # To generate a sequence, run a trained cell in a loop passing as input and input state\n",
    "    # respectively the output and output state from the previous iteration.\n",
    "    results = []\n",
    "    for i in range(run_length):\n",
    "        Yout_ = np.reshape(Yout_, [1, 1, 1]) # batch of a single sequence of a single vector with one element\n",
    "        feed = {Hin: H_, samples: Yout_, dropout_pkeep: 1.0} # no dropout during inference\n",
    "        Yout_, H_ = sess.run([Yout, H], feed_dict=feed)\n",
    "        results.append(Yout_[0,0])\n",
    "        \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tensorflow session\n",
    "This resets all neuron weights and biases to initial random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first input state\n",
    "Hzero = np.zeros([BATCHSIZE, RNN_CELLSIZE * NLAYERS])\n",
    "# variable initialization\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run([init])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "You can re-execute this cell to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 500\n",
    "\n",
    "H_ = Hzero\n",
    "losses = []\n",
    "indices = []\n",
    "for i, (next_samples, next_labels, epoch) in enumerate(utils_batching.rnn_minibatch_sequencer(data, BATCHSIZE, SEQLEN, nb_epochs=NB_EPOCHS)):\n",
    "    next_samples = np.expand_dims(next_samples, axis=2) # model wants 3D inputs [BATCHSIZE, SEQLEN, 1] \n",
    "    next_labels = np.expand_dims(next_labels, axis=2)\n",
    "\n",
    "    feed = {Hin: H_, samples: next_samples, labels: next_labels, dropout_pkeep: DROPOUT_PKEEP}\n",
    "    Yout_, H_, loss_, _, Yr_ = sess.run([Yout, H, loss, train_op, Yr], feed_dict=feed)\n",
    "    # print progress\n",
    "    if i%30 == 0:\n",
    "        print(\"epoch \" + str(epoch) + \", batch \" + str(i) + \", loss=\" + str(np.mean(loss_)))\n",
    "    if i%10 == 0:\n",
    "        losses.append(np.mean(loss_))\n",
    "        indices.append(i)\n",
    "    if i%100 == 0:\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.plot(next_labels[0,:,0])\n",
    "        plt.plot(Yr_[0,:,0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(ymax=np.amax(losses[1:])) # ignore first value for scaling\n",
    "plt.plot(indices, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMELEN=1000\n",
    "RUNLEN=500\n",
    "OFFSET=310\n",
    "\n",
    "prime_data = data[OFFSET:OFFSET+PRIMELEN]\n",
    "\n",
    "results = prediction_run(prime_data, RUNLEN)\n",
    "\n",
    "disp_data = data[OFFSET:OFFSET+PRIMELEN+RUNLEN]\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.subplot(211)\n",
    "plt.text(PRIMELEN,2.5,\"DATA |\", color=colors[1], horizontalalignment=\"right\")\n",
    "plt.text(PRIMELEN,2.5,\"| PREDICTED\", color=colors[0], horizontalalignment=\"left\")\n",
    "displayresults = np.ma.array(np.concatenate((np.zeros([PRIMELEN]), results)))\n",
    "displayresults = np.ma.masked_where(displayresults == 0, displayresults)\n",
    "plt.plot(displayresults)\n",
    "displaydata = np.ma.array(np.concatenate((prime_data, np.zeros([RUNLEN]))))\n",
    "displaydata = np.ma.masked_where(displaydata == 0, displaydata)\n",
    "plt.plot(displaydata)\n",
    "plt.subplot(212)\n",
    "plt.text(PRIMELEN,2.5,\"DATA |\", color=colors[1], horizontalalignment=\"right\")\n",
    "plt.text(PRIMELEN,2.5,\"| +PREDICTED\", color=colors[0], horizontalalignment=\"left\")\n",
    "plt.plot(displayresults)\n",
    "plt.plot(disp_data)\n",
    "RMSELEN=128\n",
    "plt.axvspan(PRIMELEN, PRIMELEN+RMSELEN, color='grey', alpha=0.1, ymin=0.05, ymax=0.95)\n",
    "plt.show()\n",
    "\n",
    "rmse = math.sqrt(np.mean((data[OFFSET+PRIMELEN:OFFSET+PRIMELEN+RMSELEN] - results[:RMSELEN])**2))\n",
    "print(\"RMSE on {} predictions (shaded area): {}\".format(RMSELEN, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
